## Credits

We kindly thank (in random order)
* [Artem Babenko](https://research.yandex.com/people/102794) and 
  [Vladimir Aliev](https://ru.linkedin.com/in/vladimir-aliev-19b93282) (yandex research) for helpful discussions
  and editorial review of the paper,
* [Jacob R. Steeves](https://github.com/unconst) (bittensor) for discussions on RPC frameworks and NAT traversal and 
  peer-to-peer technologies. 
* [Dmitry Afanasiev](https://www.linkedin.com/in/dmitry-afanasiev-295a231/) (yandex) for his guidance on networking
  and communication technologies,
* [Lidi Zheng](https://github.com/lidizheng) (google) and grpc-aio contributors for their awesome framework and [this pr](https://github.com/grpc/grpc/pull/23265)
* [Brian Muller](https://github.com/bmuller/kademlia) (parallel markets) for his implementations of [kademlia](https://github.com/bmuller/kademlia) and [rpcudp](https://github.com/bmuller/rpcudp)  
* Alexander Sherbakov (itsoft) for helpful discussions on PC and server component architecture,
* Our early adopters, [contributors](https://github.com/learning-at-home/hivemind/graphs/contributors), and reviewers


### Related projects

We also want to reference several projects that have similar ideas in mind:

* [BitTensor](https://github.com/opentensor/BitTensor) - a decentralized deep learning ecosystem with with incentive
 mechanism. Like hivemind, but peers are getting rewarded for their contribution to other peers.
  _(note: as of 26.08.2020 the project is in the early stages development)_.
* [GShard](https://arxiv.org/abs/2006.16668) - a paper by Dmitry Lepikhin et al that demonstrate the effectiveness
  of huge Mixture-of-Experts models on conventional hpc hardware. Those guys train models 4 times the size of GPT-3 on thousands of TPUv3.
* Also doing research in decentralized deep learning? Let us know!